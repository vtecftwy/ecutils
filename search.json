[
  {
    "objectID": "01_ipython.html",
    "href": "01_ipython.html",
    "title": "ipython",
    "section": "",
    "text": "System and CLI\n\nsource\n\nrun_cli\n\n run_cli (cmd:str='ls -l')\n\nRuns a cli command from jupyter notebook and print the shell output message\nUses subprocess.run with passed command to run the cli command\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncmd\nstr\nls -l\ncommand to execute in the cli\n\n\n\n\nrun_cli('pwd')\n\n/home/vtec/projects/ec-packages/ecutilities/nbs-dev\n\n\n\n\n\n\nNotebook setup\n\nsource\n\nnb_setup\n\n nb_setup (autoreload:bool=True, paths:List[str|pathlib.Path]=None)\n\nUse in first cell of notebook to set autoreload, and add system paths\nAlways add a path to the directoruy ‘src’ if srs directory exists at the same level as the nbs directory.\nWhen the notebook is not located in a tree including the name nbs, src directory is searched at the same level as the directory in which the notebook is located.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nautoreload\nbool\nTrue\nTrue to set autoreload in this notebook\n\n\npaths\nList[str | Path]\nNone\nPaths to add to the path environment variable\n\n\n\nBy default, ipython.nb_setup() - loads and set autoreload - adds a path to a directory named src when it exists at the same level as where the notebook directory is located. It no such src directory exists, no path is added\nipython.nb_setup assumes the following file structure:\n    project_directory\n          |--- nbs\n          |     | --- current_nb.ipynb\n          |     | --- ...\n          |\n          |--- src\n          |     | --- module_to_import.py\n          |     | --- ...\n          |\n          |--- data\n          |     |\n          |     | ...\nFor other file structure, specify paths as a list of Path\nBefore running nb_setup, sys.path does not include the path to the local source directory. After running it, it will be added, unless the directory does not exist.\n\nsys.path\n\n['/home/vtec/projects/ec-packages/ecutilities/nbs-dev',\n '/home/vtec/miniconda3/envs/ecutils/lib/python310.zip',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/lib-dynload',\n '',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n '/home/vtec/projects/ec-packages/ecutilities']\n\n\n\nnb_setup(autoreload=False)\n\nAdded path: /home/vtec/projects/ec-packages/ecutilities/src\n\n\n\nsys.path\n\n['/home/vtec/projects/ec-packages/ecutilities/src',\n '/home/vtec/projects/ec-packages/ecutilities/nbs-dev',\n '/home/vtec/miniconda3/envs/ecutils/lib/python310.zip',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/lib-dynload',\n '',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n '/home/vtec/projects/ec-packages/ecutilities']\n\n\nWe also can add other specific paths:\n\npath_to_add = str(Path('../nbs').resolve().absolute())\nnb_setup(autoreload=False, paths=[path_to_add])\n\nAdded path: /home/vtec/projects/ec-packages/ecutilities/nbs\n\n\n\nsys.path\n\n['/home/vtec/projects/ec-packages/ecutilities/src',\n '/home/vtec/projects/ec-packages/ecutilities/nbs',\n '/home/vtec/projects/ec-packages/ecutilities/nbs-dev',\n '/home/vtec/miniconda3/envs/ecutils/lib/python310.zip',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/lib-dynload',\n '',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages',\n '/home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n '/home/vtec/projects/ec-packages/ecutilities']\n\n\n\nsource\n\n\ninstall_code_on_cloud\n\n install_code_on_cloud (package_name:str, quiet:bool=False)\n\npip install the project code package, when nb is running in the cloud.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npackage_name\nstr\n\nproject package name, e.g. metagentools or git+https://github.com/repo.git@main\n\n\nquiet\nbool\nFalse\ninstall quietly with Trud\n\n\n\nWhen using colab, kaggle or another cloud VM, specicif code must be installed every time from the Python Package Index (PyPI) or its GitHub repo.\nWhen running locally, the project code should be pre-installed as part of the environment\n\ninstall_code_on_cloud(package_name='metagentools');\n\nThe notebook is running locally, will not automatically install project code\n\n\n\n\n\nImprove output cell formats\n\nsource\n\ndisplay_mds\n\n display_mds (*strings:str|tuple[str])\n\nDisplay one or several strings formatted in markdown format\n\ndisplay_mds('**bold** and _italic_')\n\nbold and italic\n\n\n\ndisplay_mds('**bold** and _italic_',\n            '- bullet',\n            '- bullet',\n            '&gt; Note: this is a note'\n)\n\nbold and italic\n\n\n\nbullet\n\n\n\n\nbullet\n\n\n\n\nNote: this is a note\n\n\n\n\nsource\n\n\ndisplay_dfs\n\n display_dfs (*dfs:pandas.core.frame.DataFrame)\n\nDisplay one or several pd.DataFrame in a single cell output\n\ndf1 = pd.DataFrame(data=np.random.normal(size=(10,5)))\ndf2 = pd.DataFrame(data=np.random.normal(size=(20,10)))\n\ndisplay_dfs(df1.head(3), df2.head(3))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n-0.404293\n-1.959411\n-0.136628\n0.727151\n-0.595115\n\n\n1\n2.404413\n1.958637\n-0.513681\n-1.408594\n0.950348\n\n\n2\n1.684596\n-2.020807\n0.395364\n2.704780\n-1.106685\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-2.236748\n-1.984643\n1.505316\n-0.778855\n0.461898\n-1.497143\n-0.533513\n0.450035\n0.119140\n-0.567164\n\n\n1\n1.016743\n-0.181036\n-0.593569\n-1.842810\n1.858056\n0.480069\n0.266183\n1.229341\n0.365643\n-0.231381\n\n\n2\n-0.567689\n1.072291\n0.484437\n-0.224742\n0.624904\n-1.132879\n1.338664\n-0.931461\n-0.035472\n-0.873919\n\n\n\n\n\n\n\n\nsource\n\n\npandas_nrows_ncols\n\n pandas_nrows_ncols (nrows:int|None=None, ncols:int|None=None)\n\nContext manager that sets the max number of rows and cols to apply to any output within the context\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint | None\nNone\nmax number of rows to show; show all rows if None\n\n\nncols\nint | None\nNone\nmax number of columns to show; show all columns if None\n\n\n\nWith no context manager, the pandas object are displayed with a maximum of 60 rows and 20 columns.\n\ndf = pd.DataFrame(np.random.randint(low=0, high=100, size=(3,50)))\ndisplay(df)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n\n0\n78\n57\n39\n53\n91\n46\n7\n0\n83\n92\n...\n92\n49\n11\n69\n56\n93\n51\n26\n76\n81\n\n\n1\n31\n25\n45\n99\n82\n52\n4\n97\n75\n59\n...\n19\n12\n78\n55\n11\n75\n97\n78\n97\n64\n\n\n2\n38\n91\n62\n45\n95\n97\n97\n35\n46\n24\n...\n6\n64\n31\n59\n0\n83\n17\n21\n54\n47\n\n\n\n\n3 rows × 50 columns\n\n\n\nUsing the context manager, all rows and columns will be displayed\n\nwith pandas_nrows_ncols():\n    display(df)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n\n0\n78\n57\n39\n53\n91\n46\n7\n0\n83\n92\n0\n30\n68\n6\n14\n26\n28\n94\n1\n15\n83\n64\n78\n19\n26\n91\n64\n66\n26\n66\n38\n24\n61\n32\n70\n19\n97\n69\n97\n2\n92\n49\n11\n69\n56\n93\n51\n26\n76\n81\n\n\n1\n31\n25\n45\n99\n82\n52\n4\n97\n75\n59\n43\n63\n0\n23\n86\n25\n62\n76\n44\n31\n1\n68\n3\n60\n64\n93\n91\n13\n90\n96\n96\n32\n74\n74\n53\n29\n11\n45\n48\n76\n19\n12\n78\n55\n11\n75\n97\n78\n97\n64\n\n\n2\n38\n91\n62\n45\n95\n97\n97\n35\n46\n24\n89\n17\n95\n89\n57\n85\n18\n68\n94\n91\n40\n77\n66\n95\n98\n19\n65\n82\n89\n45\n75\n18\n35\n60\n65\n53\n7\n37\n96\n74\n6\n64\n31\n59\n0\n83\n17\n21\n54\n47\n\n\n\n\n\n\n\nIt is also possible to specifically define the number of rows and columns to display\n\nwith pandas_nrows_ncols(nrows=2, ncols=6):\n    display(df)\n\n\n\n\n\n\n\n\n0\n1\n2\n...\n47\n48\n49\n\n\n\n\n0\n78\n57\n39\n...\n26\n76\n81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2\n38\n91\n62\n...\n21\n54\n47\n\n\n\n\n3 rows × 50 columns\n\n\n\n\nwith pandas_nrows_ncols(2,6):\n    print(df)\n\n    0   1   2   ...  47  48  49\n0   78  57  39  ...  26  76  81\n..  ..  ..  ..  ...  ..  ..  ..\n2   38  91  62  ...  21  54  47\n\n[3 rows x 50 columns]\n\n\n\nTechnical background\nthe context manager uses pandas’s options API\n\n\npd.options.display.max_rows, pd.options.display.max_columns\n\n(60, 20)\n\n\n\npd.get_option('display.max_rows'), pd.get_option('display.max_columns')\n\n(60, 20)\n\n\n\npd.describe_option('display.max_rows')\n\ndisplay.max_rows : int\n    If max_rows is exceeded, switch to truncate view. Depending on\n    `large_repr`, objects are either centrally truncated or printed as\n    a summary view. 'None' value means unlimited.\n\n    In case python/IPython is running in a terminal and `large_repr`\n    equals 'truncate' this can be set to 0 and pandas will auto-detect\n    the height of the terminal and print a truncated object which fits\n    the screen height. The IPython notebook, IPython qtconsole, or\n    IDLE do not run in a terminal and hence it is not possible to do\n    correct auto-detection.\n    [default: 60] [currently: 60]\n\n\n\npd.options.display.max_rows = 10\npd.reset_option('display.max_rows')\npd.options.display.max_rows\n\n60\n\n\n\nsource\n\n\ndisplay_full_df\n\n display_full_df\n                  (df:pandas.core.frame.DataFrame|pandas.core.series.Serie\n                  s)\n\nDisplay a pandas DataFrame or Series showing all rows and columns\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame | pd.Series\nDataFrame or Series to display\n\n\n\n\ndf = pd.DataFrame(np.random.randint(low=0, high=100, size=(3,50)))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n\n0\n11\n70\n3\n84\n10\n33\n54\n77\n5\n70\n...\n12\n77\n41\n30\n70\n70\n73\n84\n27\n86\n\n\n1\n8\n93\n0\n34\n68\n74\n71\n36\n69\n33\n...\n58\n98\n42\n91\n66\n30\n18\n66\n51\n89\n\n\n2\n24\n75\n2\n94\n63\n88\n49\n83\n91\n14\n...\n15\n17\n24\n74\n19\n64\n57\n92\n23\n70\n\n\n\n\n3 rows × 50 columns\n\n\n\n\ndisplay_full_df(df)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n\n0\n11\n70\n3\n84\n10\n33\n54\n77\n5\n70\n81\n24\n93\n56\n3\n74\n86\n92\n7\n49\n87\n33\n28\n53\n82\n43\n76\n52\n55\n28\n99\n0\n26\n1\n51\n67\n68\n87\n56\n64\n12\n77\n41\n30\n70\n70\n73\n84\n27\n86\n\n\n1\n8\n93\n0\n34\n68\n74\n71\n36\n69\n33\n79\n75\n78\n42\n72\n32\n50\n60\n91\n0\n86\n26\n13\n37\n23\n48\n74\n38\n52\n77\n71\n30\n3\n48\n22\n46\n2\n92\n38\n48\n58\n98\n42\n91\n66\n30\n18\n66\n51\n89\n\n\n2\n24\n75\n2\n94\n63\n88\n49\n83\n91\n14\n16\n65\n65\n16\n37\n92\n67\n90\n47\n40\n39\n50\n7\n61\n93\n18\n43\n86\n6\n25\n39\n39\n91\n75\n43\n13\n11\n88\n6\n91\n15\n17\n24\n74\n19\n64\n57\n92\n23\n70\n\n\n\n\n\n\n\n\nmsg = 'should raise a TypeError'\ncontains = 'df must me a pandas `DataFrame` or `Series`'\n\ntest_fail(display_full_df, args=['a string'], msg=msg, contains=contains)"
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "core",
    "section": "",
    "text": "Validation tools\n\nsource\n\nis_type\n\n is_type (obj:Any, obj_type:type, raise_error:bool=False)\n\nValidate that obj is of type obj_type. Raise error in the negative when raise_error is True\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobj\nAny\n\nobject whose type to validate\n\n\nobj_type\ntype\n\nexpected type for obj\n\n\nraise_error\nbool\nFalse\nwhen True, raise a ValueError is obj is not of the right type\n\n\nReturns\nbool\n\nTrue when obj is of the right type, False otherwise\n\n\n\n\nis_type(obj='this is a string', obj_type=str)\n\nTrue\n\n\n\nis_type(obj=np.ones(shape=(2,2)), obj_type=np.ndarray)\n\nTrue\n\n\n\nsource\n\n\nvalidate_path\n\n validate_path (path:str|pathlib.Path, path_type:str='file',\n                raise_error:bool=False)\n\nValidate that path is a Path or str and points to a real file or directory\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n\npath to validate\n\n\npath_type\nstr\nfile\ntype of the target path: 'file', 'dir' or 'any'\n\n\nraise_error\nbool\nFalse\nwhen True, raise a ValueError is path does not a file\n\n\nReturns\nbool\n\nTrue when path is a valid path, False otherwise\n\n\n\n\npath_file = Path('../data/img/IMG_001_512px.jpg')\nvalidate_path(path_file)\n\nTrue\n\n\n\nvalidate_path(path_file, path_type='any')\n\nTrue\n\n\n\npath_dir = Path('../data')\nvalidate_path(path_dir, path_type='dir')\n\nTrue\n\n\n\nvalidate_path(path_dir, path_type='any')\n\nTrue\n\n\n\npath_error = Path('../data/img/IIIMG_001_512px.jpg')\nvalidate_path(path_error)\n\nFalse\n\n\n\nsource\n\n\nsafe_path\n\n safe_path (path:str|pathlib.Path)\n\nReturn a Path object when given a valid path as a string or a Path, raise error otherwise\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr | Path\npath to validate\n\n\nReturns\nPath\nvalidated path returned as a pathlib.Path\n\n\n\n\n\n\nAccess key files and directories\n\nsource\n\nget_config_value\n\n get_config_value (section:str, key:str,\n                   path_to_config_file:pathlib.Path|str=None)\n\nReturns the value corresponding to the key-value pair in the configuration file (configparser format)\nWhen no path_to_config_file is provided, the function will try to find the file in: the system’s home, the parent directory of the current directory, and the Google drive directory mounted to the Colab environment.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsection\nstr\n\nsection in the configparser cfg file\n\n\nkey\nstr\n\nkey in the selected section\n\n\npath_to_config_file\nPath | str\nNone\npath to the cfg file\n\n\nReturns\nAny\n\nthe value corresponding to section&gt;key&gt;value\n\n\n\nBy defaults (path_to_config_file is None), it is assumed that the configuration file is located in the private-accross-accounts directory on google drive. If not, a path to the file (Path or str) must be provided.\nThe configuration file is expected to be in the format used by the standard module configparser documentation\n    [DEFAULT]\n    key = value\n\n    [section_name]\n    key = value\n\n    [section_name]\n    key = value\n\npath2cfg = Path('../config-sample.cfg').resolve()\nassert path2cfg.is_file(), f\"{path2cfg} is not a file\"\n\nwith open(path2cfg, 'r') as fp:\n    print(fp.read())\n\n[azure]\nazure-api-key= dummy_api_key_for_azure\n\n[kaggle]\nkaggle_username = not_my_real_kaggle_name\nkaggle_key = dummy_api_key_for_kaggle\n\n[wandb]\napi_key = dummy_api_key_for_wandb\n\n\n\n\nvalue = get_config_value(section='azure', key='azure-api-key', path_to_config_file=path2cfg)\nassert value == 'dummy_api_key_for_azure'\n\nUsing config file at /home/vtec/projects/ec-packages/ecutilities/config-sample.cfg\n\n\n\nvalue = get_config_value(section='kaggle', key='kaggle_username', path_to_config_file=path2cfg)\nassert value == 'not_my_real_kaggle_name'\n\nUsing config file at /home/vtec/projects/ec-packages/ecutilities/config-sample.cfg\n\n\n\nvalue = get_config_value(section='wandb', key='api_key', path_to_config_file=path2cfg)\nassert value == 'dummy_api_key_for_wandb'\n\nUsing config file at /home/vtec/projects/ec-packages/ecutilities/config-sample.cfg\n\n\n\nvalue = get_config_value(section='dummy', key='dummy-user-id')\nassert value.startswith('dummy-userID-from')\n\nUsing config file at /home/vtec/config-api-keys.cfg\n\n\n\n\n\nSetup utilities\n\nsource\n\nCurrentMachine\n\n CurrentMachine (*args, **kwargs)\n\nCallable class to represent info on the current machine. When called, instance return a dict all attrs:\n\nos\nhome path\nis_local, is_colab, is_kaggle\np2config path to the config file\npackage_root path to the root of the package root directory\n\n\nmachine = CurrentMachine()\nmachine()\n\n{'os': 'linux',\n 'home': Path('/home/vtec'),\n 'is_local': True,\n 'is_colab': False,\n 'is_kaggle': False,\n 'p2config': Path('/home/vtec/.ecutilities/ecutilities.cfg'),\n 'package_root': Path('/home/vtec/projects/ec-packages/ecutilities')}\n\n\n\nmachine.is_local, machine.is_colab, machine.is_kaggle\n\n(False, False, False)\n\n\nThis machine is not registered a local machine, but is also not running in the cloud. We should register it as a local machine with register_as_local\n\nsource\n\n\nCurrentMachine.register_as_local\n\n CurrentMachine.register_as_local ()\n\nUpdate the configuration file to register the machine as local machine\nUse this method to register the current machine as local machine. Only needs to be used once on a machine. Do not use on cloud VMs\n\nmachine.register_as_local()\nmachine.is_local, machine.is_colab, machine.is_kaggle\n\n(True, False, False)\n\n\n\nTechnical Note:\nThe configuration file is located at a standard location, which varies depending on the OS:\n\nWindows:\n\nhome is C:\\Users\\username\napplication data in C:\\Users\\username\\AppData/Local/... or C:\\Users\\username\\AppData\\Roaming\\... (see StackExchange)\napplication also can be loaded under a dedicated directory under C:\\Users\\username like C:\\Users\\username\\.conda\\...\n\nLinux:\n\nhome is /home/username\napplication data in a file or dedicated directory /home/username/ s.a.:\n\nfile in home directory, e.g. .gitconfig\nfile in an application dedicated directory, e.g. /home/username/.conda/...\n\n\n\necutilities places the configuration file in a dedicated directory in the home directory: - C:\\Users\\username\\.ecutilities\\ecutilities.cfg - /home/username/.ecutilities/ecutilities.cfg\nRetrieve the OS:\nsys.platform\nwin32           with Windows\nlinux           with linux\ndarwin          with macOs\nAccessing the correct path depending on the OS:\nPath().home().absolute()\nWindowsPath('C:/Users/username') with Windows\nPath('/home/username')           with linux\n\n\nsource\n\n\nProjectFileSystem\n\n ProjectFileSystem (*args, **kwargs)\n\nClass representing the project file system and key subfolders (data, nbs, src)\nSet paths to key directories, according to whether the code is running locally or in the cloud. Give access to path to these key folders and information about the environment.\n\npfs = ProjectFileSystem()\npfs()\n\n{'os': 'linux',\n 'home': Path('/home/vtec'),\n 'is_local': True,\n 'is_colab': False,\n 'is_kaggle': False,\n 'p2config': Path('/home/vtec/.ecutilities/ecutilities.cfg'),\n 'package_root': Path('/home/vtec/projects/ec-packages/ecutilities')}\n\n\n\nsource\n\n\nProjectFileSystem.create_project_file_system\n\n ProjectFileSystem.create_project_file_system (p2project_root,\n                                               overwrite=False)\n\nCreate a standard project file system with the following structure:\n    project_root\n        |--- data   all data files\n        |--- nbs    all notebooks for work and experiments\n        |--- src    all scripts and code\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np2project_root\n\n\npath to project root, where all subfolder will be located\n\n\noverwrite\nbool\nFalse\noverwrite current folders if they exist when True (not implemented yet)\n\n\n\n\npfs.create_project_file_system(Path('/home/vtec/projects/ec-packages/ecutilities'))\n\n/home/vtec/projects/ec-packages/ecutilities/data\n/home/vtec/projects/ec-packages/ecutilities/nbs\n/home/vtec/projects/ec-packages/ecutilities/src\nCreated project file system in /home/vtec/projects/ec-packages/ecutilities\n\n\n\n\n\nFile structure exploration\n\nsource\n\nfiles_in_tree\n\n files_in_tree (path:str|pathlib.Path, pattern:str|None=None)\n\nList files in directory and its subdiretories, print tree starting from parent directory\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n\npath to the directory to scan\n\n\npattern\nstr | None\nNone\npattern (glob style) to match in file name to filter the content\n\n\n\n\np2dir = Path('').resolve()\nprint(p2dir, '\\n')\n\nfiles = files_in_tree(p2dir)\nprint(f\"List of {len(files)} files when unfiltered\")\n\n/home/vtec/projects/ec-packages/ecutilities/nbs-dev \n\necutilities\n  |--nbs-dev\n  |    |--0_02_plotting.ipynb (0)\n  |    |--2_01_image_utils.ipynb (1)\n  |    |--1_01_eda_stats_utils.ipynb (2)\n  |    |--0_01_ipython.ipynb (3)\n  |    |--0_00_core.ipynb (4)\n  |    |--.last_checked (5)\n  |    |--sidebar.yml (6)\n  |    |--1_02_ml.ipynb (7)\n  |    |--index.ipynb (8)\n  |    |--nbdev.yml (9)\n  |    |--9_01_dev_utils.ipynb (10)\n  |    |--styles.css (11)\n  |    |--_quarto.yml (12)\n  |    |--.ipynb_checkpoints\n  |    |    |--0_02_plotting-checkpoint.ipynb (13)\n  |    |    |--9_01_dev_utils-checkpoint.ipynb (14)\n  |    |    |--0_01_ipython-checkpoint.ipynb (15)\n  |    |    |--0_00_core-checkpoint.ipynb (16)\n  |    |    |--1_01_eda_stats_utils-checkpoint.ipynb (17)\n  |    |    |--index-checkpoint.ipynb (18)\n  |    |    |--2_01_image_utils-checkpoint.ipynb (19)\n  |    |    |--1_02_ml-checkpoint.ipynb (20)\nList of 21 files when unfiltered\n\n\nUse pattern to filter the paths to return (using glob syntax)\n\nfiles = files_in_tree(p2dir, pattern='ipynb')\nprint(f\"List of {len(files)} files when filtered\")\n\necutilities\n  |--nbs-dev\n  |    |--0_02_plotting.ipynb (0)\n  |    |--2_01_image_utils.ipynb (1)\n  |    |--1_01_eda_stats_utils.ipynb (2)\n  |    |--0_01_ipython.ipynb (3)\n  |    |--0_00_core.ipynb (4)\n  |    |--1_02_ml.ipynb (5)\n  |    |--index.ipynb (6)\n  |    |--9_01_dev_utils.ipynb (7)\n  |    |--.ipynb_checkpoints\n  |    |    |--0_02_plotting-checkpoint.ipynb (8)\n  |    |    |--9_01_dev_utils-checkpoint.ipynb (9)\n  |    |    |--0_01_ipython-checkpoint.ipynb (10)\n  |    |    |--0_00_core-checkpoint.ipynb (11)\n  |    |    |--1_01_eda_stats_utils-checkpoint.ipynb (12)\n  |    |    |--index-checkpoint.ipynb (13)\n  |    |    |--2_01_image_utils-checkpoint.ipynb (14)\n  |    |    |--1_02_ml-checkpoint.ipynb (15)\nList of 16 files when filtered\n\n\n\nsource\n\n\npath_to_parent_dir\n\n path_to_parent_dir (pattern:str, path:str|pathlib.Path|None=None)\n\nClimb directory tree up to a directory starting with pattern, and return its path.\n\nWhen no directory is found in the tree starting with pattern, return the current directory path.\nIt is possible to pass a path as starting path to climb from.\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npattern\nstr\n\npattern to identify the parent directory\n\n\npath\nstr | Path | None\nNone\noptional path from where to seek for parent directory\n\n\nReturns\nPath\n\npath of the parent directory\n\n\n\n\np2dir = path_to_parent_dir('nbs')\nassert 'nbs-dev' in p2dir.parts and 'nbs' not in p2dir.parts\np2dir\n\nPath('/home/vtec/projects/ec-packages/ecutilities/nbs-dev')\n\n\n\np2dir = path_to_parent_dir('nbs', Path('../nbs/sandbox.ipynb').resolve())\nassert 'nbs' in p2dir.parts and 'nbs-dev' not in p2dir.parts\np2dir\n\nPath('/home/vtec/projects/ec-packages/ecutilities/nbs')\n\n\n\np2dir = path_to_parent_dir('not-in-tree').resolve()\nassert p2dir == Path().absolute()\n\n\np2project_root = path_to_parent_dir('ecutilities')\nassert 'ecutilities' in p2project_root.parts and 'nbs' not in p2project_root.parts\np2project_root\n\nPath('/home/vtec/projects/ec-packages/ecutilities')"
  },
  {
    "objectID": "02_plotting.html",
    "href": "02_plotting.html",
    "title": "plotting",
    "section": "",
    "text": "Work with color maps\n\nsource\n\nplot_cmap_collections\n\n plot_cmap_collections (cmap_collections:str|list[str]=None)\n\nPlot all color maps in the collections passed as cmap_collections\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncmap_collections\nstr | list[str]\nNone\nlist of color map collections to display (from cmaps.keys())\n\n\n\nThe following color map collections are defined:\n\n\n- Cyclic\n- Diverging\n- Miscellaneous\n- Perceptually Uniform Sequential\n- Qualitative\n- Sequential\n- Sequential (2)\n\n\nplot_cmap_collections will plot a color bar for each color map in the selected collections:\n\nA single collection\n\n\nplot_cmap_collections('Cyclic')\n\n\n\n\n\nSeveral collections\n\n\nplot_cmap_collections(['Qualitative', 'Sequential'])\n\n\n\n\n\n\n\n\nAll the collections\n\n\nplot_cmap_collections()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_color_bar\n\n plot_color_bar (cmap:str, series:list[int|float]=None)\n\nPlot a color bar with value overlay from series based on cmap\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncmap\nstr\n\nstring name of one of the cmaps\n\n\nseries\nlist[int | float]\nNone\nseries of numerical values to show for each color\n\n\n\n\nplot_color_bar('tab10', range(10))\n\n\n\n\n\nplot_color_bar('tab10', series=range(6))\n\n\n\n\n\nplot_color_bar('tab10', series=[0, 1, 2])\n\n\n\n\n\nsource\n\n\nget_color_mapper\n\n get_color_mapper (series:list[int|float], cmap:str='tab10')\n\nReturn color mapper based on a color map and a series of values\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseries\nlist[int | float]\n\nseries of values to map to colors\n\n\ncmap\nstr\ntab10\nname of the cmap to use\n\n\n\n\nUsage\nThis function is used to ensure coherent colors for different plots.\n\nDefine a color mapper based on values and cmap: clr_mapper = get_color_mapper([1, 2, 3, 4], cmap='Paired)\nCall the color mapper and have it return the appropriate values for any plot: clr_mapper.to_rgba(2)\n\n\nExample\nWe have dataset with several features.\n\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn.datasets import make_blobs\n\n\nn_feats = 6\ncol_list = [f\"col_{i}\" for i in range(n_feats)]\nX, y = make_blobs(n_samples=5_000, n_features=n_feats, centers=10, shuffle=True)\n\nX = pd.DataFrame(X, columns=col_list)\nX.head(3)\n\n\n\n\n\n\n\n\ncol_0\ncol_1\ncol_2\ncol_3\ncol_4\ncol_5\n\n\n\n\n0\n-4.377115\n-3.113744\n4.417737\n7.327412\n7.366114\n0.033885\n\n\n1\n-10.319655\n-6.998589\n1.126784\n7.731522\n4.524063\n-1.337312\n\n\n2\n1.542669\n6.398550\n8.267037\n-1.024028\n-0.697208\n8.599691\n\n\n\n\n\n\n\n\n\n1. Define a color mapper based on values and cmap\nWe cluster the data into 10 clusters and make a scatter plot of two of the features, displaying the 10 cluster using a cmap.\nTo ensure that we can keep the same cluster color mapping for other plots, we use clr_mapper to predefine how colors are mapped to each cluster:\n\nclr_mapper = get_color_mapper(cluster_ids, cmap=cmap).\n\n\nclustering = KMeans(n_clusters=10)\nclusters = clustering.fit_predict(X)\ncluster_ids = np.unique(clusters)\n\n\ncmap='tab10'\n\nplt.figure(figsize=(6, 3))\nplt.scatter(X.col_0, X.col_1, c=clusters, s=2, cmap=cmap)\nplt.colorbar()\nplt.title('two first features data points, colored by cluster value')\nplt.show()\n\n\n\n\n\nclr_mapper = get_color_mapper(cluster_ids, cmap=cmap)\n\n\n\nCall the color mapper and use it in any plot\nUse for another plot, showing another feature, and its value for each sample, colored according to its cluster\n\nfeatname = 'col_4'\nplt.figure(figsize=(12, 3))\nplt.plot(X[featname], c='grey', alpha=.25, lw=0.25)\nplt.title(f'{featname}.')\nplt.show()\n\nplt.figure(figsize=(12, 3))\nplt.plot(X[featname], c='grey', alpha=.25, lw=0.25)\nfor c in cluster_ids:\n    mask = y == c\n    X[f\"{featname}_{c}\"] = X.loc[:, featname]\n    X.loc[~mask, f\"{featname}_{c}\"] = np.nan\n    plt.plot(X[f\"{featname}_{c}\"], label=str(c), c=clr_mapper.to_rgba(c), lw=0, marker='o', markersize=1)\nplt.title(f'{featname}. Data points colored according to the cluster it belongs to.')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced plots\n\nsource\n\nplot_feature_scatter\n\n plot_feature_scatter (X:numpy.ndarray, y:Optional[numpy.ndarray]=None,\n                       n_plots:int=2, axes_per_row:int=3, axes_size:int=5)\n\nPlots n_plots scatter plots of randomly selected combinations of two features out of X\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nnp.ndarray\n\ninput dataset. X.shape[1] is used to set the total number of features\n\n\ny\nOptional[np.ndarray]\nNone\ntarget dataset\n\n\nn_plots\nint\n2\nnumber of feature pairs scatter plot to show\n\n\naxes_per_row\nint\n3\nnumber of axes per row. number of rows will be calculated accordingly\n\n\naxes_size\nint\n5\nsize of one axes. figsize will be (ncols * axes_size, nrows * axes_size)\n\n\n\n\nX.shape\n\n(5000, 16)\n\n\n\nn_feats = 6\ncol_list = [f\"col_{i}\" for i in range(n_feats)]\nX, y = make_blobs(n_samples=5_000, n_features=n_feats, centers=10, shuffle=True)\n\nX = pd.DataFrame(X, columns=col_list)\n\nplot_feature_scatter(X.values, y, n_plots=6, axes_per_row=3, axes_size=5)\n\n\n\n\nWhen not value is available for y, it is set to 1 by default\n\nplot_feature_scatter(X.values, n_plots=4, axes_per_row=2, axes_size=2)"
  },
  {
    "objectID": "01_image_utils.html",
    "href": "01_image_utils.html",
    "title": "image_utils",
    "section": "",
    "text": "Image metadata\n\nsource\n\nget_date_from_file_name\n\n get_date_from_file_name (path2file:pathlib.Path, date_pattern:str=None)\n\nRetrieve the date from the file name.\nReturns the date encoded in the file name (as per date_pattern) as a datetime. Default date pattern is YY-MM-DD, i.g. regex ^(-(-(*\nreturns: date in datetime format, if found. False if not found or if file does not exist\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath2file\nPath\n\nPath to the file\n\n\ndate_pattern\nstr\nNone\nregex pattern for the date, if None, default date pattern is YY-MM-DD\n\n\nReturns\ndt.datetime\n\n\n\n\n\n\np2img = Path('../data/img/22-06-11_IMG_512px.jpg')\nget_date_from_file_name(path2file=p2img)\n\ndatetime.datetime(2022, 6, 11, 0, 0)\n\n\n\nsource\n\n\ndate_is_within_year\n\n date_is_within_year (date, year)\n\nTrue if the passed date (datetime) is within year, False otherwise\n\nsource\n\n\nexif2dt\n\n exif2dt (exif_d)\n\nTransform a date in bytes format from EXIF into datetime format\nexif_d: date in exif format, i.e. bytes. E.g. b”2018:11:21” returns: the date in datetime or False if no date is detected or not well formatted\n\nsource\n\n\nadd_missing_dates_to_exif\n\n add_missing_dates_to_exif (path2folder:pathlib.Path, year:int=None,\n                            maxhours:int=24,\n                            do_not_update_exif:bool=False,\n                            verbose:bool=False)\n\nAdd missing EXIF original and digitized dates, based on file creation or date in file name.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath2folder\nPath\n\nPath to the folder holding all jpg photos to handle\n\n\nyear\nint\nNone\nyear used to filter all dates\n\n\nmaxhours\nint\n24\nmaximum acceptable difference (in hours) between exif dates and file dates\n\n\ndo_not_update_exif\nbool\nFalse\nwhen Trud, prevent updating the exif file; used in debugging or testing\n\n\nverbose\nbool\nFalse\nwhen True, print original, updated and retrieved updated EXIF info\n\n\n\n\nLogic of the function:\nIn order to better control the data changes and avoid mistaken exif updates, the process is done on a year by year basis, i.e. a specific year needs to be passed to the function and only dates within the passed year will be updated. All other dates will be disregarded.\n\nRetrieve EXIF info from image\nWhen there is no EXIF.DatetimeOriginal in the image EXIF:\n\nuse date from file name if exists, else\nuse date from creation or modification, whichever is earlier\n\nWhen there is an EXIF.DatetimeOriginal, compare with date from file, if any:\n\nif difference &lt; maxhours, do nothing\nif difference &gt;= maxhours, use date from filename\n\nIf the date extracted from file name or file creation/modification date is not in passed year, skipped any change"
  },
  {
    "objectID": "02_ml.html",
    "href": "02_ml.html",
    "title": "ml",
    "section": "",
    "text": "Reference for kaggle API: https://github.com/Kaggle/kaggle-api\n\nWorking with datasets\n\nsource\n\nare_features_consistent\n\n are_features_consistent (df1:pandas.core.frame.DataFrame,\n                          df2:pandas.core.frame.DataFrame,\n                          dependent_variables:list[str]=None,\n                          raise_error:bool=False)\n\nVerify that features/columns in training and test sets are consistent\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf1\npd.DataFrame\n\nFirst set, typically the training set\n\n\ndf2\npd.DataFrame\n\nSecond set, typically the test set or inference set\n\n\ndependent_variables\nlist[str]\nNone\nList of column name(s) for dependent variables\n\n\nraise_error\nbool\nFalse\nTrue to raise an error if not consistent\n\n\nReturns\nbool\n\nTrue if features in train and test datasets are consistent, False otherwise\n\n\n\nTraining set and test set should have the same features/columns, except for the dependent variable(s). This function tests whether this is the case.\n\nfeats = [f\"Feature_{i:02d}\" for i in range(10)]\nX_train = pd.DataFrame(np.random.normal(size=(500, 10)), columns=feats)\nX_test = pd.DataFrame(np.random.normal(size=(100, 10)), columns=feats)\nX_test_not_consistant = X_test.iloc[:, 2:]\ndisplay(X_train.head(3))\ndisplay(X_test.head(3))\ndisplay(X_test_not_consistant.head(3))\n\n\n\n\n\n\n\n\nFeature_00\nFeature_01\nFeature_02\nFeature_03\nFeature_04\nFeature_05\nFeature_06\nFeature_07\nFeature_08\nFeature_09\n\n\n\n\n0\n1.394439\n0.266156\n-0.070705\n-0.462835\n0.025394\n0.361311\n0.801035\n0.205413\n0.941988\n2.868571\n\n\n1\n0.740853\n-1.390509\n-1.583919\n-1.951328\n-0.739606\n0.775896\n-0.060068\n0.121640\n0.864439\n1.192721\n\n\n2\n0.526661\n0.233771\n1.028485\n0.284115\n-0.448474\n0.512852\n-0.673979\n0.426295\n-0.181841\n0.455442\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature_00\nFeature_01\nFeature_02\nFeature_03\nFeature_04\nFeature_05\nFeature_06\nFeature_07\nFeature_08\nFeature_09\n\n\n\n\n0\n-1.612301\n-0.659610\n-0.553156\n0.477722\n0.498676\n-2.585540\n1.329870\n-1.638286\n-0.248535\n-1.322088\n\n\n1\n0.857624\n1.224392\n0.115925\n-0.055684\n-1.336148\n3.651585\n0.532247\n-1.325887\n-0.616351\n-1.350044\n\n\n2\n0.381214\n-0.024726\n0.853689\n0.270990\n-0.571249\n-0.117136\n-1.895106\n-0.176482\n-0.331920\n0.671925\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature_02\nFeature_03\nFeature_04\nFeature_05\nFeature_06\nFeature_07\nFeature_08\nFeature_09\n\n\n\n\n0\n-0.553156\n0.477722\n0.498676\n-2.585540\n1.329870\n-1.638286\n-0.248535\n-1.322088\n\n\n1\n0.115925\n-0.055684\n-1.336148\n3.651585\n0.532247\n-1.325887\n-0.616351\n-1.350044\n\n\n2\n0.853689\n0.270990\n-0.571249\n-0.117136\n-1.895106\n-0.176482\n-0.331920\n0.671925\n\n\n\n\n\n\n\nCompare all the features/columns\n\nare_features_consistent(X_train, X_test)\n\nTrue\n\n\n\nare_features_consistent(X_train, X_test_not_consistant)\n\nFalse\n\n\nare_features_consistent(X_train, X_test_not_consistant, raise_error=True) should raise an error instead of returning False\n\ntest_fail(\n    f=are_features_consistent, \n    args=(X_train, X_test_not_consistant),\n    kwargs = {'raise_error':True},\n    contains=\"Discrepancy between training and test feature set:\",\n    msg=f\"Should raise a ValueError\"\n)\n\nWhen comparing training and inference set, the training set will have more features as it includes the dependant variables. To test the consistency of the datasets, specify whith columns are dependant variables.\nFor instance, X_train has all features, including the two dependant variables Feature_08 and Feature_09.\n\nX_inference = X_train.iloc[:, :-2]\ndisplay(X_train.head(3))\ndisplay(X_inference.head(3))\n\n\n\n\n\n\n\n\nFeature_00\nFeature_01\nFeature_02\nFeature_03\nFeature_04\nFeature_05\nFeature_06\nFeature_07\nFeature_08\nFeature_09\n\n\n\n\n0\n1.394439\n0.266156\n-0.070705\n-0.462835\n0.025394\n0.361311\n0.801035\n0.205413\n0.941988\n2.868571\n\n\n1\n0.740853\n-1.390509\n-1.583919\n-1.951328\n-0.739606\n0.775896\n-0.060068\n0.121640\n0.864439\n1.192721\n\n\n2\n0.526661\n0.233771\n1.028485\n0.284115\n-0.448474\n0.512852\n-0.673979\n0.426295\n-0.181841\n0.455442\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature_00\nFeature_01\nFeature_02\nFeature_03\nFeature_04\nFeature_05\nFeature_06\nFeature_07\n\n\n\n\n0\n1.394439\n0.266156\n-0.070705\n-0.462835\n0.025394\n0.361311\n0.801035\n0.205413\n\n\n1\n0.740853\n-1.390509\n-1.583919\n-1.951328\n-0.739606\n0.775896\n-0.060068\n0.121640\n\n\n2\n0.526661\n0.233771\n1.028485\n0.284115\n-0.448474\n0.512852\n-0.673979\n0.426295\n\n\n\n\n\n\n\n\nare_features_consistent(X_train, X_inference, dependent_variables=['Feature_08', 'Feature_09'])\n\nTrue\n\n\n\n\n\nKaggle\n\nsource\n\nkaggle_setup_colab\n\n kaggle_setup_colab (path_to_config_file:pathlib.Path|str=None)\n\nUpdate kaggle API and create security key json file from config file on Google Drive\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath_to_config_file\nPath | str\nNone\npath to the configuration file (e.g. config.cfg)\n\n\n\n\nTechnical Background\nReferences: Kaggle API documentation\nKaggle API Token to be placed as a json file at the following location:\n    ~/.kaggle/kaggle.json\n    %HOMEPATH%\\.kaggle\\kaggle.json\nTo access Kaggle with API, a security key needs to be placed in the correct location on colab.\nconfig.cfg file must include the following lines:\n    [kaggle]\n    kaggle_username = kaggle_user_name\n    kaggle_key = API key provided by kaggle\nInfo on how to get an api key (kaggle.json) here\n\nsource\n\n\n\nkaggle_list_files\n\n kaggle_list_files (code:str=None, mode:str='competitions')\n\nList all files available in the competition or dataset for the passed code\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncode\nstr\nNone\ncode for the kaggle competition or dataset\n\n\nmode\nstr\ncompetitions\nmode: competitions or datasets\n\n\n\n\nsource\n\n\nkaggle_download_competition_files\n\n kaggle_download_competition_files (competition_code:str=None,\n                                    train_files:[]=[], test_files:list=[],\n                                    submit_files:list=[],\n                                    project_folder:str='ds')\n\ndownload all files for passed competition, unzip them if required, move them to train, test and submit folders\ncompetition_code: str code of the kaggle competition train_files: list of str names of files to be moved into train folder test_files: list of str names of files to be moved into test folder submit_files: list of str names of files to be moved into submit folder\n\n\n\nOthers\n\nsource\n\nfastbook_on_colab\n\n fastbook_on_colab ()\n\nSet up environment to run fastbook notebooks for colab\nCode extracted from fastbook notebook:\n# Install fastbook and dependencies\n!pip install -Uqq fastbook\n\n# Load utilities and install them\n!wget -O utils.py https://raw.githubusercontent.com/vtecftwy/fastbook/walk-thru/utils.py\n!wget -O fastbook_utils.py https://raw.githubusercontent.com/vtecftwy/fastbook/walk-thru/fastbook_utils.py\n\nfrom fastbook_utils import *\nfrom utils import *\n\n# Setup My Drive\nsetup_book()\n\n# Download images and code required for this notebook\nimport os\nos.makedirs('images', exist_ok=True)\n!wget -O images/chapter1_cat_example.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/master/nbs/images/chapter1_cat_example.jpg\n!wget -O images/cat-01.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/cat-01.jpg\n!wget -O images/cat-02.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/cat-02.jpg\n!wget -O images/dog-01.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/dog-01.jpg\n!wget -O images/dog-02.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/dog-01.jpg"
  },
  {
    "objectID": "01_dev_utils.html",
    "href": "01_dev_utils.html",
    "title": "dev_utils",
    "section": "",
    "text": "Classes and decorators to work with sys.settrace(tracefunc)\n\n\ntracefunc(the trace function) should have three arguments: frame, event, and arg:\n\nframe is the current stack frame.\nevent is a string: 'call', 'line', 'return', 'exception' or 'opcode'.\narg depends on the event type.\n\nframe has many attributes, including those below which are used in the tracing classes below:\n\n\n\n\n\n\n\n\nType\nAttribute\nDescription\n\n\n\n\nframe\nf_back\nnext outer frame object (this frame’s caller)\n\n\n\nf_code\ncode object being executed in this frame\n\n\n\nf_lineno\ncurrent line number in Python source code\n\n\ncode\nco_code\nstring of raw compiled bytecode\n\n\n\nco_filename\nname of file in which this code object was created\n\n\n\nco_name\nname with which this code object was defined\n\n\n\nco_names\ntuple of names other than arguments and function locals\n\n\n\nco_stacksize\nvirtual machine stack space required\n\n\n\nco_varnames\ntuple of names of arguments and local variables\n\n\n\nSee full documentation in sys and inspect built-in modules.\nExperiments with frame and its attributes\n\ndef fn(a):\n    return a + 1\n\nb = fn(1)\n\nframes = sys._current_frames()\n\n\nframesids = list(frames.keys())\n\nfor id in framesids:\n    fr = frames[id]\n    print(f\"{fr.f_lineno:5d}   {fr.f_code.co_name:20s}   {fr.f_code.co_filename:50s}\")\n\n   37   run                    /home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages/ipykernel/parentpoller.py\n  330   wait                   /home/vtec/miniconda3/envs/ecutils/lib/python3.10/threading.py\n  482   select                 /home/vtec/miniconda3/envs/ecutils/lib/python3.10/selectors.py\n  327   _watch_pipe_fd         /home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages/ipykernel/iostream.py\n  327   _watch_pipe_fd         /home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages/ipykernel/iostream.py\n  103   run                    /home/vtec/miniconda3/envs/ecutils/lib/python3.10/site-packages/ipykernel/heartbeat.py\n  482   select                 /home/vtec/miniconda3/envs/ecutils/lib/python3.10/selectors.py\n    6   &lt;cell line: 6&gt;         /tmp/ipykernel_7075/1808055364.py                 \n\n\n\nframe = frames[framesids[-1]]\n\nprint(frame.f_code.co_names)\nprint(frame.f_code.co_stacksize)\nprint(frame.f_code.co_varnames)\n\nprint(inspect.getsource(frame))\n\n('sys', '_current_frames', 'frames')\n2\n()\ndef fn(a):\n    return a + 1\n\n\n\n\n\n\n\nsource\n\n\n\n StackTrace (with_call:bool=True, with_return:bool=True,\n             with_exception:bool=True, max_depth:int=-1)\n\nCallable class acting as tracefunc to capture and print information on all stack frame being run\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwith_call\nbool\nTrue\nwhen True, call events are traced\n\n\nwith_return\nbool\nTrue\nwhen True, return events are traced\n\n\nwith_exception\nbool\nTrue\nwhen True, exceptions events are traced\n\n\nmax_depth\nint\n-1\nmaximum depth of the trace, default is full depth\n\n\n\n\nsource\n\n\n\n\n StackTrace.__call__ (frame:inspect.FrameInfo, event:str, arg:Any)\n\ntracefuncused in sys.settrace(tracefunc)\n\n\n\n\nType\nDetails\n\n\n\n\nframe\ninspect.FrameInfo\nframe argument in tracefunc\n\n\nevent\nstr\nevent argument in tracefunc\n\n\narg\nAny\narg argument in tracefunc\n\n\n\n\nsource\n\n\n\n\n StackTrace.print_stack_info (co_filename:str|pathlib.Path, ret:bool,\n                              depth:int)\n\nThis methods can be overloaded to customize what is printed out\n\n\n\n\nType\nDetails\n\n\n\n\nco_filename\nstr | Path\ncode file name\n\n\nret\nbool\n\n\n\ndepth\nint\ndepth\n\n\n\n\nsource\n\n\n\n\n StackTraceJupyter (with_call:bool=True, with_return:bool=True,\n                    with_exception:bool=True, max_depth:int=-1)\n\nPrint stack frame information in Jupyter notebook context (filters out jupyter overhead)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwith_call\nbool\nTrue\nwhen True, call events are traced\n\n\nwith_return\nbool\nTrue\nwhen True, return events are traced\n\n\nwith_exception\nbool\nTrue\nwhen True, exceptions events are traced\n\n\nmax_depth\nint\n-1\nmaximum depth of the trace, default is full depth\n\n\n\n\n\n\n\n\nsource\n\n\n\n stack_trace (**kw)\n\nstack_trace decorator function\n\nsource\n\n\n\n\n stack_trace_jupyter (**kw)\n\nstack_trace_jupyter decorator function\n\n\nSeveral functions, some of them nested and some of them with errors.\n\ndef empty_func():\n        pass\n\ndef call_empty_and_return_zero():\n    empty_func()\n    return 0\n\ndef divide_by_zero_error():\n    1/0\n\ndef decrement_recursion(i):\n    if i == 0:\n        return\n    decrement_recursion(i-1)\n\nUsing the @stack_trace or @stack_trace_jupyter decorator allows a detailled trace, function by function and identify where it fails.\n\n@stack_trace(with_return=True, with_exception=True, max_depth=10)\ndef function_to_trace():\n    call_empty_and_return_zero()\n    decrement_recursion(5)\n    divide_by_zero_error()\n\n\ntest_fail(\n    function_to_trace,\n    msg='Should raise a div by 0 exception',\n    contains='division by zero'\n)\n\nfunction_to_trace   [call]  in /tmp/ipykernel_7075/661382525.py line:1\n  call_empty_and_return_zero    [call]  in /tmp/ipykernel_7075/2653264264.py line:4\n    empty_func  [call]  in /tmp/ipykernel_7075/2653264264.py line:1\n    empty_func  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:2\n  call_empty_and_return_zero    [return]    0   in /tmp/ipykernel_7075/2653264264.py line:6\n  decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n    decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n      decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n        decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n          decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:13\n          decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n        decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n      decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n    decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  divide_by_zero_error  [call]  in /tmp/ipykernel_7075/2653264264.py line:8\n  divide_by_zero_error  [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/2653264264.py line:9\n  divide_by_zero_error  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:9\nfunction_to_trace   [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/661382525.py line:5\nfunction_to_trace   [return]    None    in /tmp/ipykernel_7075/661382525.py line:5\n\n\n\n@stack_trace_jupyter(with_return=True, with_exception=True, max_depth=15)\ndef function_to_trace_jupyter():\n    call_empty_and_return_zero()\n    decrement_recursion(5)\n    divide_by_zero_error()\n\n\ntest_fail(\n    function_to_trace_jupyter,\n    msg='Should raise a div by 0 exception',\n    contains='division by zero'\n)\n\nfunction_to_trace_jupyter   [call]  in /tmp/ipykernel_7075/2571630784.py line:1\n  call_empty_and_return_zero    [call]  in /tmp/ipykernel_7075/2653264264.py line:4\n    empty_func  [call]  in /tmp/ipykernel_7075/2653264264.py line:1\n    empty_func  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:2\n  call_empty_and_return_zero    [return]    0   in /tmp/ipykernel_7075/2653264264.py line:6\n  decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n    decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n      decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n        decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n          decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:13\n          decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n        decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n      decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n    decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  divide_by_zero_error  [call]  in /tmp/ipykernel_7075/2653264264.py line:8\n  divide_by_zero_error  [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/2653264264.py line:9\n  divide_by_zero_error  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:9\nfunction_to_trace_jupyter   [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/2571630784.py line:5\nfunction_to_trace_jupyter   [return]    None    in /tmp/ipykernel_7075/2571630784.py line:5"
  },
  {
    "objectID": "01_dev_utils.html#tracing-classes",
    "href": "01_dev_utils.html#tracing-classes",
    "title": "dev_utils",
    "section": "",
    "text": "source\n\n\n\n StackTrace (with_call:bool=True, with_return:bool=True,\n             with_exception:bool=True, max_depth:int=-1)\n\nCallable class acting as tracefunc to capture and print information on all stack frame being run\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwith_call\nbool\nTrue\nwhen True, call events are traced\n\n\nwith_return\nbool\nTrue\nwhen True, return events are traced\n\n\nwith_exception\nbool\nTrue\nwhen True, exceptions events are traced\n\n\nmax_depth\nint\n-1\nmaximum depth of the trace, default is full depth\n\n\n\n\nsource\n\n\n\n\n StackTrace.__call__ (frame:inspect.FrameInfo, event:str, arg:Any)\n\ntracefuncused in sys.settrace(tracefunc)\n\n\n\n\nType\nDetails\n\n\n\n\nframe\ninspect.FrameInfo\nframe argument in tracefunc\n\n\nevent\nstr\nevent argument in tracefunc\n\n\narg\nAny\narg argument in tracefunc\n\n\n\n\nsource\n\n\n\n\n StackTrace.print_stack_info (co_filename:str|pathlib.Path, ret:bool,\n                              depth:int)\n\nThis methods can be overloaded to customize what is printed out\n\n\n\n\nType\nDetails\n\n\n\n\nco_filename\nstr | Path\ncode file name\n\n\nret\nbool\n\n\n\ndepth\nint\ndepth\n\n\n\n\nsource\n\n\n\n\n StackTraceJupyter (with_call:bool=True, with_return:bool=True,\n                    with_exception:bool=True, max_depth:int=-1)\n\nPrint stack frame information in Jupyter notebook context (filters out jupyter overhead)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwith_call\nbool\nTrue\nwhen True, call events are traced\n\n\nwith_return\nbool\nTrue\nwhen True, return events are traced\n\n\nwith_exception\nbool\nTrue\nwhen True, exceptions events are traced\n\n\nmax_depth\nint\n-1\nmaximum depth of the trace, default is full depth"
  },
  {
    "objectID": "01_dev_utils.html#tracing-decorators",
    "href": "01_dev_utils.html#tracing-decorators",
    "title": "dev_utils",
    "section": "",
    "text": "source\n\n\n\n stack_trace (**kw)\n\nstack_trace decorator function\n\nsource\n\n\n\n\n stack_trace_jupyter (**kw)\n\nstack_trace_jupyter decorator function\n\n\nSeveral functions, some of them nested and some of them with errors.\n\ndef empty_func():\n        pass\n\ndef call_empty_and_return_zero():\n    empty_func()\n    return 0\n\ndef divide_by_zero_error():\n    1/0\n\ndef decrement_recursion(i):\n    if i == 0:\n        return\n    decrement_recursion(i-1)\n\nUsing the @stack_trace or @stack_trace_jupyter decorator allows a detailled trace, function by function and identify where it fails.\n\n@stack_trace(with_return=True, with_exception=True, max_depth=10)\ndef function_to_trace():\n    call_empty_and_return_zero()\n    decrement_recursion(5)\n    divide_by_zero_error()\n\n\ntest_fail(\n    function_to_trace,\n    msg='Should raise a div by 0 exception',\n    contains='division by zero'\n)\n\nfunction_to_trace   [call]  in /tmp/ipykernel_7075/661382525.py line:1\n  call_empty_and_return_zero    [call]  in /tmp/ipykernel_7075/2653264264.py line:4\n    empty_func  [call]  in /tmp/ipykernel_7075/2653264264.py line:1\n    empty_func  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:2\n  call_empty_and_return_zero    [return]    0   in /tmp/ipykernel_7075/2653264264.py line:6\n  decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n    decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n      decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n        decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n          decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:13\n          decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n        decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n      decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n    decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  divide_by_zero_error  [call]  in /tmp/ipykernel_7075/2653264264.py line:8\n  divide_by_zero_error  [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/2653264264.py line:9\n  divide_by_zero_error  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:9\nfunction_to_trace   [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/661382525.py line:5\nfunction_to_trace   [return]    None    in /tmp/ipykernel_7075/661382525.py line:5\n\n\n\n@stack_trace_jupyter(with_return=True, with_exception=True, max_depth=15)\ndef function_to_trace_jupyter():\n    call_empty_and_return_zero()\n    decrement_recursion(5)\n    divide_by_zero_error()\n\n\ntest_fail(\n    function_to_trace_jupyter,\n    msg='Should raise a div by 0 exception',\n    contains='division by zero'\n)\n\nfunction_to_trace_jupyter   [call]  in /tmp/ipykernel_7075/2571630784.py line:1\n  call_empty_and_return_zero    [call]  in /tmp/ipykernel_7075/2653264264.py line:4\n    empty_func  [call]  in /tmp/ipykernel_7075/2653264264.py line:1\n    empty_func  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:2\n  call_empty_and_return_zero    [return]    0   in /tmp/ipykernel_7075/2653264264.py line:6\n  decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n    decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n      decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n        decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n          decrement_recursion   [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [call]  in /tmp/ipykernel_7075/2653264264.py line:11\n            decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:13\n          decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n        decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n      decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n    decrement_recursion [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  decrement_recursion   [return]    None    in /tmp/ipykernel_7075/2653264264.py line:14\n  divide_by_zero_error  [call]  in /tmp/ipykernel_7075/2653264264.py line:8\n  divide_by_zero_error  [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/2653264264.py line:9\n  divide_by_zero_error  [return]    None    in /tmp/ipykernel_7075/2653264264.py line:9\nfunction_to_trace_jupyter   [exception] &lt;class 'ZeroDivisionError'&gt; in /tmp/ipykernel_7075/2571630784.py line:5\nfunction_to_trace_jupyter   [return]    None    in /tmp/ipykernel_7075/2571630784.py line:5"
  },
  {
    "objectID": "01_eda_stats_utils.html",
    "href": "01_eda_stats_utils.html",
    "title": "eda_stats_utils",
    "section": "",
    "text": "Data analysis plots\n\nsource\n\necdf\n\n ecdf (data:pandas.core.series.Series|numpy.ndarray,\n       threshold:Optional[int]=None,\n       figsize:Optional[tuple[int,int]]=None)\n\nCompute Empirical Cumulative Distribution Function (ECDF), plot it and returns values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\npd.Series | np.ndarray\n\ndata to analyse\n\n\nthreshold\nOptional[int]\nNone\ncummulative frequency used as threshold. Must be between 0 and 1\n\n\nfigsize\nOptional[tuple[int, int]]\nNone\nfigure size (width, height)\n\n\nReturns\ntuple[np.array, np.array, int]\n\nsorted data (ascending), cumulative frequencies, last index\n\n\n\necdf plots the empirical cumulative distribution function (ECDF), for data cumulative frequencies from 0 to threshold &lt;= 1.\nThe empirical cumulative distribution function (ECDF) is a step function that jumps up by 1/n at each of the n data points in the dataset. Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value.\nThe ECDF is an estimate of the cumulative distribution function that generated the points in the sample. It allows to compare with the distribution that is expected.\n\ndf = pd.DataFrame(data={'a': np.random.random(100) * 100,'b': np.random.random(100) * 50,'c': np.random.random(100)})\ndata_1, freq_1, last_idx_1 = ecdf(data=df.a, threshold=1, figsize=(5, 5))\n\n\n\n\n\ndata_2, freq_2, last_idx_2 = ecdf(data=df.a, threshold=0.75, figsize=(5, 5))\n\n\n\n\n\ndata_3, freq_3, last_idx_3 = ecdf(data=df.a, threshold=0.5, figsize=(5, 5))\n\n\n\n\nThe ecdf function also returns: - the data used for the ECDF, with values sorted from smallest to largest - the respective cummulative frequencies - the index of data value/frequency plotted (at the threshold)\n\ndata_1\n\narray([ 0.85,  2.1 ,  2.44,  3.22,  3.24,  3.9 ,  4.65,  5.53,  6.08,  6.34,  6.84,  6.98,  8.37,  9.09, 12.58, 12.96,\n       13.62, 14.5 , 15.06, 17.11, 17.52, 17.99, 19.12, 22.3 , 22.45, 23.87, 23.87, 24.15, 24.28, 25.95, 26.79, 30.31,\n       30.4 , 34.83, 38.01, 38.56, 38.62, 39.47, 39.58, 41.1 , 42.43, 43.57, 43.73, 45.92, 47.19, 47.66, 48.52, 49.03,\n       49.6 , 49.98, 51.07, 52.14, 53.06, 53.92, 54.66, 55.88, 56.12, 56.29, 56.52, 57.02, 57.63, 58.54, 59.37, 62.65,\n       62.91, 62.93, 63.22, 64.12, 64.82, 65.06, 65.35, 67.16, 70.6 , 70.94, 72.24, 72.45, 73.87, 74.91, 78.44, 79.17,\n       79.78, 79.82, 82.53, 84.96, 85.14, 85.43, 86.19, 86.35, 88.31, 89.76, 90.42, 91.69, 93.11, 96.71, 97.38, 98.32,\n       98.33, 98.37, 98.56, 98.83])\n\n\n\nfreq_1\n\narray([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18,\n       0.19, 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36,\n       0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72,\n       0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ,\n       0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  ])\n\n\nThe function returns all the sorted data and frequencies, independently from the threshold. In the example above, data_1 and data_2 have the same values.\n\nnp.array_equal(data_1, data_2), np.array_equal(freq_1, freq_2)\n\n(True, True)\n\n\n\ndata_2[:last_idx_2+1]\n\narray([ 0.85,  2.1 ,  2.44,  3.22,  3.24,  3.9 ,  4.65,  5.53,  6.08,  6.34,  6.84,  6.98,  8.37,  9.09, 12.58, 12.96,\n       13.62, 14.5 , 15.06, 17.11, 17.52, 17.99, 19.12, 22.3 , 22.45, 23.87, 23.87, 24.15, 24.28, 25.95, 26.79, 30.31,\n       30.4 , 34.83, 38.01, 38.56, 38.62, 39.47, 39.58, 41.1 , 42.43, 43.57, 43.73, 45.92, 47.19, 47.66, 48.52, 49.03,\n       49.6 , 49.98, 51.07, 52.14, 53.06, 53.92, 54.66, 55.88, 56.12, 56.29, 56.52, 57.02, 57.63, 58.54, 59.37, 62.65,\n       62.91, 62.93, 63.22, 64.12, 64.82, 65.06, 65.35, 67.16, 70.6 , 70.94, 72.24])\n\n\n\nsource\n\n\ncluster_columns\nPlot dendogram based on Dataframe’s columns’ spearman correlation coefficients\n\nThis function was first seen on fastai repo\n\n\nfeats = [f\"Feature_{i:02d}\" for i in range(10)]\nprint('Features:')\nprint(', '.join(feats))\nX = pd.DataFrame(np.random.normal(size=(500, 10)), columns=feats)\ncluster_columns(X, (6, 4), 8)\n\nFeatures:\nFeature_00, Feature_01, Feature_02, Feature_03, Feature_04, Feature_05, Feature_06, Feature_07, Feature_08, Feature_09"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ecutilities",
    "section": "",
    "text": "PyPi\npip install ecutilities\nGitHub\nTwo options to get versions which are not yet released on PyPi:\n\nclone the github repo and install it in edit mode from within the cloned repo:\n\npip install -e .\n\ninstall with pip from the github repo directly for hosted virtual machines:\n\npip install git+https://github.com/vtecftwy/ecutils.git@master\npip install git+https://github.com/vtecftwy/ecutils.git@develop"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "ecutilities",
    "section": "",
    "text": "PyPi\npip install ecutilities\nGitHub\nTwo options to get versions which are not yet released on PyPi:\n\nclone the github repo and install it in edit mode from within the cloned repo:\n\npip install -e .\n\ninstall with pip from the github repo directly for hosted virtual machines:\n\npip install git+https://github.com/vtecftwy/ecutils.git@master\npip install git+https://github.com/vtecftwy/ecutils.git@develop"
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "ecutilities",
    "section": "Modules:",
    "text": "Modules:\n\nGeneral use:\n\ncore\nipython\nplotting\n\nData Science and Machine learning:\n\neda_stats_utils\nml\n\nImages\n\nimage_utils\n\nDevelopment\n\ndev_utils"
  }
]