[
  {
    "objectID": "dev_utils.html",
    "href": "dev_utils.html",
    "title": "dev_utils - Development Utilities",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "dev_utils.html#usage",
    "href": "dev_utils.html#usage",
    "title": "dev_utils - Development Utilities",
    "section": "Usage:",
    "text": "Usage:\nSeveral functions, some of them nested and some of them with errors.\n\ndef foo():\n        pass\n\ndef bar():\n    foo()\n    return 0\n\ndef error():\n    1/0\n\ndef recur(i):\n    if i == 0:\n        return\n    recur(i-1)\n\nUsing the @stack_trace or @stack_trace_jupyter decorator allows a detailled view the trace function by function and where it fails.\n\n@stack_trace(with_return=True, with_exception=True, max_depth=3)\ndef test():\n    bar()\n    recur(5)\n    error()\n    \n# test_fail(test(), contains='division by zero')\n\ntry:\n    test()\nexcept ZeroDivisionError:\n    print('message error will appear here')\n\ntest    [call]  in /tmp/ipykernel_4396/3507459406.py line:1\n  bar   [call]  in /tmp/ipykernel_4396/2052304305.py line:4\n    foo [call]  in /tmp/ipykernel_4396/2052304305.py line:1\n    foo [return]    None    in /tmp/ipykernel_4396/2052304305.py line:2\n  bar   [return]    0   in /tmp/ipykernel_4396/2052304305.py line:6\n  recur [call]  in /tmp/ipykernel_4396/2052304305.py line:11\n    recur   [call]  in /tmp/ipykernel_4396/2052304305.py line:11\n      recur [call]  in /tmp/ipykernel_4396/2052304305.py line:11\n      recur [return]    None    in /tmp/ipykernel_4396/2052304305.py line:14\n    recur   [return]    None    in /tmp/ipykernel_4396/2052304305.py line:14\n  recur [return]    None    in /tmp/ipykernel_4396/2052304305.py line:14\n  error [call]  in /tmp/ipykernel_4396/2052304305.py line:8\n  error [exception] <class 'ZeroDivisionError'> in /tmp/ipykernel_4396/2052304305.py line:9\n  error [return]    None    in /tmp/ipykernel_4396/2052304305.py line:9\ntest    [exception] <class 'ZeroDivisionError'> in /tmp/ipykernel_4396/3507459406.py line:5\ntest    [return]    None    in /tmp/ipykernel_4396/3507459406.py line:5\nmessage error will appear here\n\n\n\n@stack_trace_jupyter(with_return=True, with_exception=True, max_depth=3)\ndef test_jupyter():\n    bar()\n    recur(5)\n    error()\n\ntry:\n    test_jupyter()\nexcept ZeroDivisionError:\n    print('message error will appear here')\n\ntest_jupyter    [call]  in /tmp/ipykernel_4396/2932405012.py line:1\n  bar   [call]  in /tmp/ipykernel_4396/2052304305.py line:4\n    foo [call]  in /tmp/ipykernel_4396/2052304305.py line:1\n    foo [return]    None    in /tmp/ipykernel_4396/2052304305.py line:2\n  bar   [return]    0   in /tmp/ipykernel_4396/2052304305.py line:6\n  recur [call]  in /tmp/ipykernel_4396/2052304305.py line:11\n    recur   [call]  in /tmp/ipykernel_4396/2052304305.py line:11\n      recur [call]  in /tmp/ipykernel_4396/2052304305.py line:11\n      recur [return]    None    in /tmp/ipykernel_4396/2052304305.py line:14\n    recur   [return]    None    in /tmp/ipykernel_4396/2052304305.py line:14\n  recur [return]    None    in /tmp/ipykernel_4396/2052304305.py line:14\n  error [call]  in /tmp/ipykernel_4396/2052304305.py line:8\n  error [exception] <class 'ZeroDivisionError'> in /tmp/ipykernel_4396/2052304305.py line:9\n  error [return]    None    in /tmp/ipykernel_4396/2052304305.py line:9\ntest_jupyter    [exception] <class 'ZeroDivisionError'> in /tmp/ipykernel_4396/2932405012.py line:5\ntest_jupyter    [return]    None    in /tmp/ipykernel_4396/2932405012.py line:5\nmessage error will appear here"
  },
  {
    "objectID": "plotting.html",
    "href": "plotting.html",
    "title": "plotting module - Functions for images",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "plotting.html#usage",
    "href": "plotting.html#usage",
    "title": "plotting module - Functions for images",
    "section": "Usage",
    "text": "Usage\nThis function is used to assure coherent colors for different plots. 1. Define a color mapper based on values and cmap: clr_mapper = get_color_mapper([1, 2, 3, 4], cmap='Paired) 2. Call the color mapper and have it return the appropriate values for a plot: clr_mapper.to_rgba(2)\nExample\nWe have dataset with several features. 1. We cluster the data into 10 clusters and show two of the features clustered (first plot) 2. We want to show how other features in the dataset are broken down with respect to the clusters\nTo do that, we need to use exactly the same color between the plots. get_color_mapper utility function makes this easier.\n\nn_feats = 6\ncol_list = [f\"col_{i}\" for i in range(n_feats)]\nX, y = make_blobs(n_samples=5_000, n_features=n_feats, centers=10, shuffle=True)\n\nX = pd.DataFrame(X, columns=col_list)\n\ncmap='tab10'\n\nclustering = KMeans(n_clusters=10)\nclusters = clustering.fit_predict(X)\ncluster_ids = np.unique(clusters)\n\nclr_mapper = get_color_mapper(cluster_ids, cmap=cmap)\n\nplt.figure(figsize=(6, 3))\nplt.scatter(X.col_0, X.col_1, c=clusters, s=2, cmap=cmap)\nplt.colorbar()\nplt.title('two first features data points, colored by cluster value')\nplt.show()\n\n#   Plot another feature, colored according to the clustering\nplt.figure(figsize=(12, 3))\nfeatname = 'col_4'\nplt.plot(X[featname], c='grey', alpha=.25, lw=0.25)\nfor c in cluster_ids:\n    mask = y == c\n    X[f\"{featname}_{c}\"] = X.loc[:, featname]\n    X.loc[~mask, f\"{featname}_{c}\"] = np.nan\n    plt.plot(X[f\"{featname}_{c}\"], label=str(c), c=clr_mapper.to_rgba(c), lw=0, marker='o', markersize=2)\nplt.title(f'{featname} where each data point is highligthed according to cluster it belongs to.')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nsource\n\nplot_feature_scatter\nPlots n_plots scatter plots of random combinations of two features out of X\nRandomly selects n_plots pairs out of all possible combinations of features pairs for the dataset X.\nX, y: the dataset. X.shape[1] is used to set the total number of features n_plots: the number of feature pairs to plot as a scatter plot axes_per_row: the number of axes per row to plot. number of rows will be calculated accordingly default value is 3 axes per row axes_size: the size of one axes. figsize will be (ncols * axes_size, nrows * axes_size) default value is 5\n\nX.shape\n\n(5000, 16)\n\n\n\nn_feats = 6\ncol_list = [f\"col_{i}\" for i in range(n_feats)]\nX, y = make_blobs(n_samples=5_000, n_features=n_feats, centers=10, shuffle=True)\n\nX = pd.DataFrame(X, columns=col_list)\n\nplot_feature_scatter(X.values, y, n_plots=6, axes_per_row=3, axes_size=5)\n\n\n\n\nWhen not value is available for y, it is set to 1 by default\n\nplot_feature_scatter(X.values, n_plots=6, axes_per_row=3, axes_size=5)"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "ecutils",
    "section": "",
    "text": "Reference for kaggle API: https://github.com/Kaggle/kaggle-api\n\nsource\n\nare_features_consistent\n\n are_features_consistent (train_df, test_df, dependent_variables=None)\n\nVerifies that features in training and test sets are consistent\nTraining set and test set should have the same features/columns, except for the dependent variables\ntrain_dr: pd.DataFrame training dataset test_df: pd.DataFrame test dataset dependent_variables: list list of column names for the dependent variables\n\nsource\n\n\ncluster_columns\n\n cluster_columns (df, figsize=(10, 6), font_size=12)\n\nPlot dendogram based on columns’ spearman correlation coefficients\nFirst seen on fastai repository\n\nsource\n\n\nrun_cli\n\n run_cli (cmd='ls -l')\n\nWrapper to use subprocess.run with passed command, and print the shell messages\ncmd: str cli command to execute\n\nsource\n\n\nget_config_value\n\n get_config_value (section, key, path_to_config_file=None)\n\nReturns the value corresponding to the key-value pair in the configuration file (configparser format)\nBy defaults, it is assumed that the configuration file is saved on google drive. If not, pass a proper Path object. The configuration file must be in the format of configparser (https://docs.python.org/3/library/configparser.html)\nParameters: section (str): name of the section where the key-value pair is stored key (str): name of the key path_to_config_file(Path or str): path to the configparser configuration file\nReturn (str): value in the key-value pair stored\n\nsource\n\n\nfastbook_on_colab\n\n fastbook_on_colab ()\n\nSet up environment to run fastbook notebooks for colab\nCode from notebook: # Install fastbook and dependencies !pip install -Uqq fastbook\n\n\nLoad utilities and install them\n!wget -O utils.py https://raw.githubusercontent.com/vtecftwy/fastbook/walk-thru/utils.py !wget -O fastbook_utils.py https://raw.githubusercontent.com/vtecftwy/fastbook/walk-thru/fastbook_utils.py\nfrom fastbook_utils import  from utils import \n\n\nSetup My Drive\nsetup_book()\n\n\nDownload images and code required for this notebook\nimport os os.makedirs(‘images’, exist_ok=True) !wget -O images/chapter1_cat_example.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/master/nbs/images/chapter1_cat_example.jpg !wget -O images/cat-01.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/cat-01.jpg !wget -O images/cat-02.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/cat-02.jpg !wget -O images/dog-01.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/dog-01.jpg !wget -O images/dog-02.jpg https://raw.githubusercontent.com/vtecftwy/fastai-course-v4/walk-thru/nbs/images/dog-01.jpg\n\nsource\n\nkaggle_setup_colab\n\n kaggle_setup_colab (path_to_config_file=None)\n\nUpdate kaggle API and create security key json file from config file on Google Drive\nKaggle API documentation: https://github.com/Kaggle/kaggle-api\nKaggle API Token to be placed as a json file at the following location: ~/.kaggle/kaggle.json %HOMEPATH%.kaggle.json\nTo access Kaggle with API, a security key needs to be placed in the correct location on colab. config.cfg file must include the following lines: [kaggle] kaggle_username = kaggle_user_name kaggle_key = API key provided by kaggle\nInfo on how to get your api key (kaggle.json) here: https://github.com/Kaggle/kaggle-api#api-credentials\npath_to_config_file: str or Path: path to the configuration file (e.g. config.cfg)\n\nsource\n\n\nkaggle_list_files\n\n kaggle_list_files (code=None, mode='competitions')\n\nList all files available in the competition or dataset for the passed code\n\nsource\n\n\nkaggle_download_competition_files\n\n kaggle_download_competition_files (competition_code=None, train_files=[],\n                                    test_files=[], submit_files=[],\n                                    project_folder='ds')\n\ndownload all files for passed competition, unzip them if required, move them to train, test and submit folders\ncompetition_code: str code of the kaggle competition train_files: list of str names of files to be moved into train folder test_files: list of str names of files to be moved into test folder submit_files: list of str names of files to be moved into submit folder"
  },
  {
    "objectID": "eda_stats_utils.html",
    "href": "eda_stats_utils.html",
    "title": "eda_stats_utils - Utility functions for EDA and Statistics",
    "section": "",
    "text": "source\n\npandas_all_cols_and_rows\n\n pandas_all_cols_and_rows (f)\n\ndecorator function to force display of all the columns in a DataFrame, only for one time\n\n\n\npandas_all_cols_and_rows..wrapper\n\n pandas_all_cols_and_rows.<locals>.wrapper (*args, **kwargs)\n\n\nsource\n\n\necdf\nCompute Empirical Cumulative Distribution Function (ECDF).\necdf plots the empirical cumulative distribution function (ECDF), for data cumulative frequencies from 0 to threshold <= 1.\n\ndf = pd.DataFrame(data={'a': np.random.random(100) * 100,'b': np.random.random(100) * 50,'c': np.random.random(100)})\ndata_1, freq_1, last_idx_1 = ecdf(data=df.a, threshold=1, figsize=(5, 5))\ndata_2, freq_2, last_idx_2 = ecdf(data=df.a, threshold=0.5, figsize=(5, 5))\n\n\n\n\n\n\n\nIt also returns: - the data used for the ECDF, with values sorted from smallest to largest - the respective cummulative frequencies - the index of data value/frequency plotted (at the threshold)\n\ndata_1\n\narray([ 1.56,  2.9 ,  3.46,  4.08,  5.32,  5.39,  7.27,  8.52, 10.61, 10.77, 11.9 , 12.84, 13.44, 13.75, 17.34, 18.49,\n       18.98, 19.7 , 20.5 , 20.81, 22.48, 22.71, 23.18, 23.42, 24.99, 25.71, 26.54, 27.61, 29.27, 29.93, 30.16, 30.54,\n       30.54, 31.95, 32.46, 32.6 , 33.06, 36.84, 38.01, 39.33, 39.75, 40.56, 40.7 , 41.44, 42.15, 43.95, 44.65, 45.68,\n       45.93, 47.09, 47.94, 48.98, 49.08, 51.66, 52.74, 54.61, 55.8 , 57.43, 57.82, 59.76, 61.09, 61.39, 62.77, 64.3 ,\n       64.73, 65.94, 65.96, 66.43, 66.66, 66.82, 67.44, 69.6 , 70.53, 74.08, 74.23, 74.24, 75.02, 76.02, 77.34, 78.84,\n       80.73, 80.84, 81.78, 83.39, 83.74, 85.85, 86.34, 87.16, 88.76, 89.16, 89.2 , 91.  , 93.08, 93.46, 94.57, 94.69,\n       96.74, 96.92, 96.98, 97.2 ])\n\n\n\nfreq_1\n\narray([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18,\n       0.19, 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36,\n       0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72,\n       0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ,\n       0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  ])\n\n\nThe function returns all the sorted data and frequencies, independently from the threshold. In the example above, data_1 and data_2 have the same values.\n\nnp.array_equal(data_1, data_2), np.array_equal(freq_1, freq_2)\n\n(True, True)\n\n\n\ndata_2[:last_idx_2+1]\n\narray([ 1.56,  2.9 ,  3.46,  4.08,  5.32,  5.39,  7.27,  8.52, 10.61, 10.77, 11.9 , 12.84, 13.44, 13.75, 17.34, 18.49,\n       18.98, 19.7 , 20.5 , 20.81, 22.48, 22.71, 23.18, 23.42, 24.99, 25.71, 26.54, 27.61, 29.27, 29.93, 30.16, 30.54,\n       30.54, 31.95, 32.46, 32.6 , 33.06, 36.84, 38.01, 39.33, 39.75, 40.56, 40.7 , 41.44, 42.15, 43.95, 44.65, 45.68,\n       45.93, 47.09])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ecutils package",
    "section": "",
    "text": "There are two options:\n\ninstall in develop mode from local source:\n\npip install -e . from the project folder, or\npip install -e \"path to local source code directory\"\n\ninstall in from github for hosted VMs:\n\npip install git+https://github.com/vtecftwy/ecutils.git@master\npip install git+https://github.com/vtecftwy/ecutils.git@develop"
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "ecutils package",
    "section": "Modules:",
    "text": "Modules:\n\nGeneral use:\n\nipython_utils\nplotting\n\nData Science and Machine learning:\n\neda_stats_utils\nml\n\nHandling of images\n\nimage_utils"
  },
  {
    "objectID": "ipython.html",
    "href": "ipython.html",
    "title": "ipython module - Utility functions for Jupyter NB",
    "section": "",
    "text": "source\n\ndisplay_mds\n\n display_mds (*strings)\n\nUtility function to display several strings formatted in markdown format\n:param strings: any number of strings with text in markdown format :return: None\n\nsource\n\n\ndisplay_dfs\n\n display_dfs (*dfs)\n\nUtility function to display several DataFrame with one operations :param dfs: any number of Pandas DataFrames :return: None\n\nsource\n\n\nnb_setup\n\n nb_setup (autoreload=True, paths=None)\n\nUse in first cell of nb for setting up autoreload, paths, …\nBy default, nb_setup() loads and set autoreload and adds a path to a directory named ‘src’ at the same level as where the notebook directory is located.\nBy default, nb_setup assumes the following file structure:\nproject_directory | — notebooks | | — current_nb.ipynb | | — … | |— src | | — scripts_to_import.py | | — … | |— data | | | | …"
  },
  {
    "objectID": "image_utils.html",
    "href": "image_utils.html",
    "title": "image_utils module - Utility functions for Images",
    "section": "",
    "text": "source\n\nget_date_from_file_name\n\n get_date_from_file_name (path2file, date_pattern=None)\n\nRetrieve the date from the file name.\nReturns the date encoded in the file name (as per date_pattern) as a datetime. Default date pattern is YY-MM-DD, i.g. regex ^(-(-(*\npath2file: pathlib.Path object pointing to the file date_pattern: the regex pattern for the date, if different from the default one returns: date in datetime format, if found. False if not found or if file does not exist\n\nsource\n\n\ndate_is_within_year\n\n date_is_within_year (date, year)\n\nTrue if the passed date (datetime) is within year, False otherwise\n\nsource\n\n\nexif2dt\n\n exif2dt (exif_d)\n\nTransform a date in bytes format from EXIF into datetime format\nexif_d: date in exif format, i.e. bytes. E.g. b”2018:11:21” returns: the date in datetime or False if no date is detected or not well formatted\n\nsource\n\n\nadd_missing_dates_to_exif\n\n add_missing_dates_to_exif (path2folder, year=None, maxhours=24,\n                            do_not_update_exif=False, verbose=False)\n\nAdd missing EXIF original and digitized dates, based on file creation or date in file name. In order to better control the data changes and avoid mistaken exif updates, the process is done on a year by year basis, i.e. a specific year needs to be passed to the function and only dates within the passed year will be updated. All other dates will be disregarded.\npath2folder: …………. pathlib.Path: object pointing to the folder holding all jpg photos to handle year: ……………….. int: year used to filter all dates maxhours: ……………. int: maximum acceptable difference (in hours) between exif dates and file dates do_not_update_exif: …… bool: flag to prevent updating the exif file, used in debugging or testing verbose: …………….. bool: flag to print original, updated and retrieved updated EXIF info\nLogic of the function: 1. Retrieve EXIF info from image 2. When there is no EXIF.DatetimeOriginal in the image EXIF: - use date from file name if exists, else - use date from creation or modification, whichever is earlier 3. When there is an EXIF.DatetimeOriginal, compare with date from file, if any: - if difference < maxhours, do nothing - if difference >= maxhours, use date from filename 4. If the date extracted from file name or file creation/modification date is not in passed year, skipped any change"
  }
]